\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{7}

\begin{document}




\chapter{Some Special Functions}
\section{Notes}
\begin{itemize}
    \item \marginnote{3/7:}Plan:
    \begin{enumerate}
        \item Go over some of the hits in chapter 8.
        \item Define sine.
        \item Power series.
        \item Exponential functions ($\log$, $\sin$, $\cos$).
    \end{enumerate}
    \item Proposition (power series properties): If $\sum_{n=0}^\infty a_nx^n$ converges for all $|x|<R$, and $f:B_R(0)\to\R$ is defined by
    \begin{equation*}
        f(x) = \sum_{n=0}^\infty a_nx^n
    \end{equation*}
    then:
    \begin{enumerate}[label={(\alph*)}]
        \item $f$ is continuous.
        \begin{itemize}
            \item From the root test, $\sum_{n=0}^\infty a_nx^n$ is in fact absolutely convergent on $(-R,R)$. Therefore, on any interval $[-R+\epsilon,R-\epsilon]$ ($0<\epsilon<R$), we have
            \begin{equation*}
                |a_nx^n| \leq |a_n||R+\epsilon|^n
            \end{equation*}
            so by the $M$-test, $\sum_{n=0}^\infty a_nx^n$ converges uniformly on $[-R+\epsilon,R-\epsilon]$. Then since $\sum_{n=0}^\infty a_nx^n$ converges uniformly on $[-R+\epsilon,R-\epsilon]$, we have (a) since all $\sum_{n=0}^Na_nx^n$ are continuous.
        \end{itemize}
        \item $f$ is differentiable with $f'(x)=\sum_{n=1}^\infty na_nx^{n-1}$.
        \begin{itemize}
            \item (b) follows similarly to (a) by uniform convergence.
            \item Note that $\limsup\sqrt[n]{|na_n|}=\limsup\sqrt[n]{|a_n|}$ (since $\lim_{n\to\infty}\sqrt[n]{n}=1$).
            \item Therefore, $\sum_{n=1}^\infty na_nx^{n-1}$ converges on $(-R,R)$.
        \end{itemize}
        \item More generally, $f$ is infinitely differentiable with
        \begin{equation*}
            f^{(k)}(x) = \sum_{n=k}^\infty\frac{n!}{(n-k)!}a_nx^{n-k}
        \end{equation*}
        \begin{itemize}
            \item Now (c) follows as in the proof of (b).
        \end{itemize}
        \item We have the identity
        \begin{equation*}
            a_k = \frac{f^{(k)}(0)}{k!}
        \end{equation*}
        \begin{itemize}
            \item (d) follows from (c) by plugging in zero.
        \end{itemize}
    \end{enumerate}
    \item Note that historically, the analysis of power series motivated the development of all of the Chapter 7 theorems; we simply learned those first without motivation to present the proofs in an ordered manner.
    \item Aside: Consider the exponential function $x^y$ for $x,y\in\R$ with $x\geq 0$.
    \begin{itemize}
        \item We define it for natural numbers and integers fairly easily, then rationals, and then for reals as the supremum of exponentials of the entries in the Dedekind cut below $x\in\R$.
        \item Under this definition, we can confirm our normal exponential rules and then that $x^y$ is continuous.
    \end{itemize}
    \item Recall that
    \begin{equation*}
        \e = \sum_{n=0}^\infty\frac{1}{n!}
    \end{equation*}
    \item So now we are going to construct $E(x)$, $L(x)$, $C(x)$, and $S(x)$ (which are just $\e[x]$, $\ln(x)$, $\cos(x)$, and $\sin(x)$).
    \item Define $E:\C\to\C$ by
    \begin{equation*}
        E(z) = \sum_{n=0}^\infty\frac{z^n}{n!}
    \end{equation*}
    \begin{itemize}
        \item By the proposition, it converges and is continuous for all $z\in\C$.
        \item For the real numbers, $E$ is differentiable. ($E$ is also complex-differentiable, but we won't go into that).
    \end{itemize}
    \item Proposition: $E(z)E(w)=E(z+w)$ for all $z,w\in\C$.
    \begin{itemize}
        \item We have by the Cauchy product (Mertens' theorem) that
        \begin{align*}
            E(z)E(w) &= \left( \sum_{n=0}^\infty\frac{z^n}{n!} \right)\left( \sum_{n=0}^\infty\frac{w^n}{n!} \right)\\
            &= \sum_{n=0}^\infty\sum_{k=0}^n\frac{z^kw^{n-k}}{k!(n-k)!}\\
            &= \sum_{n=0}^\infty\frac{1}{n!}\sum_{k=0}^n\binom{n}{k}z^kw^{n-k}\\
            &= \sum_{n=0}^\infty\frac{1}{n!}(z+w)^n\\
            &= E(z+w)
        \end{align*}
    \end{itemize}
    \item Corollary: $E(z)E(-z)=E(0)=1$ for all $z\in\C$.
    \item $E(x)>0$ for $x\geq 0$.
    \begin{itemize}
        \item It follows since $E(z+w)=E(z)E(w)$ that $E(x)>0$ for all $x\in\R$.
    \end{itemize}
    \item $\dv*{E}{x}=E$; $E$ is the unique, normalized ($E(0)=1$) function such that this is true.
    \begin{itemize}
        \item We can prove this from the power series definition.
    \end{itemize}
    \item $E(x)\to\infty$ as $x\to\infty$ and $E(x)\to 0$ as $x\to -\infty$. (Also from the power series definition.)
    \item $0\leq x_1<x_2$ implies that $E(x_1)<E(x_2)$.
    \begin{itemize}
        \item Either from $\dv*{E}{x}=E>0$ or from the power series definition.
        \item It follows from $E(z+w)=E(z)E(w)$ that $x_1<x_2$ implies $E(x_1)<E(x_2)$.
    \end{itemize}
    \item \marginnote{3/9:}Plan:
    \begin{enumerate}
        \item Keep going with $E$, $L$, $C$, and $S$.
        \item Prove the fundamental theorem of algebra.
    \end{enumerate}
    \item Define
    \begin{equation*}
        E(z) = \sum_{n=0}^\infty\frac{z^n}{n!}
    \end{equation*}
    \begin{itemize}
        \item Recall that $E(z+w)=E(z)E(w)$.
    \end{itemize}
    \item Theorem: $E(x)=\e[x]$ for all $x\in\R$.
    \begin{itemize}
        \item $E(1)=\e[1]$ (by definition).
        \item $E(n)=\e[n]$ (by $E(z+w)=E(z)E(w)$).
        \item $[E(p/q)]^q=E(p)=\e[p]$ (by $E(z+w)=E(z)E(w)$).
        \item $E(p/q)=\e[p/q]$ for all $p/q\in\Q$.
        \item $E(x)=\e[x]$ for all $x\in\R$ (since both LHS and RHS are continuous functions that agree on $\Q$).
    \end{itemize}
    \item Briefly: $E:\R\to\R^+$ is a strictly increasing surjective function. Thus, we have an inverse function $L:\R^+\to\R$.
    \item Theorem: $L$ is differentiable (and therefore continuous).
    \begin{itemize}
        \item Since $E'=E>0$ everywhere, we may apply the inverse function theorem at every point.
    \end{itemize}
    \item Now by the chain rule, $E(L(x))=x$ for all $x\in\R^+$, so taking derivatives yields
    \begin{align*}
        E'(L(x))L'(x) &= 1\\
        E(L(x))L'(x) &= 1\\
        xL'(x) &= 1\\
        L'(x) &= \frac{1}{x}
    \end{align*}
    \item Proposition:
    \begin{enumerate}
        \item $L(uw)=L(u)+L(w)$.
        \item $L(x)=\int_1^xt^{-1}\dd{t}$.
    \end{enumerate}
    \item Trig functions:
    \begin{align*}
        C(x) &= \frac{1}{2}[E(ix)+E(-ix)]&
        S(x) &= \frac{1}{2i}[E(ix)-E(-ix)]
    \end{align*}
    \begin{itemize}
        \item You can use these definitions to prove trig identities, having derived them geometrically.
    \end{itemize}
    \item Proposition: If $x\in\R$, then $C(x),S(x)\in\R$.
    \begin{itemize}
        \item Key observation: $E(\bar{z})=\overline{E(z)}$.
        \item We have
        \begin{align*}
            \overline{C(x)} &= \frac{1}{2}[\overline{E(ix)}+\overline{E(-ix)}]\\
            &= \frac{1}{2}[E(-ix)+E(ix)]\\
            &= C(x)
        \end{align*}
        \item Symmetric for $S(x)$.
    \end{itemize}
    \item Note that we could equally well define $C,S$ by
    \begin{align*}
        C(x) &= \sum_{k=0}^\infty\frac{(-1)^k}{(2k)!}x^{2k}&
        S(x) &= \sum_{k=0}^\infty\frac{(-1)^k}{(2k+1)!}x^{2k+1}
    \end{align*}
    \item Proposition: $E(ix)=C(x)+iS(x)$.
    \item Proposition: $C,S$ are differentiable with
    \begin{align*}
        C'(x) &= -S(x)&
        S'(x) &= C(x)
    \end{align*}
    \item Proposition: For all $x\in\R$, $|E(ix)|=1$.
    \begin{itemize}
        \item We have that
        \begin{equation*}
            |E(ix)|^2 = E(ix)\overline{E(ix)}
            = E(ix)E(-ix)
            = E(0)
            = 1
        \end{equation*}
        \item Taking square roots of both sides yields the desired result.
    \end{itemize}
    \item The above result proves that the imaginary axis maps onto the unit circle in the complex plane.
    \item We now define $\pi$ and all that.
    \begin{itemize}
        \item Goal: Show that for all $z\in\C$ with $|z|=1$, there exists a unique $\theta\in[0,2\pi)$ such that $\e[i\theta]=z$. Further, $E(ix)$ has period $2\pi$.
    \end{itemize}
    \item Proposition: $C(x)^2+S(x)^2=1$.
    \begin{itemize}
        \item Use $E(ix)=C(x)+iS(x)$ and $|E(ix)|=1$.
    \end{itemize}
    \item Proposition: There exists some positive number $x$ such that $C(x)=0$.
    \begin{itemize}
        \item Suppose (contradiction): $C(x)>0$ for all $x>0$ (since $C(0)=1$).
        \item Thus, $S'(x)>0$ for all $x>0$.
        \item Consequently, given $0<x<y$,
        \begin{equation*}
            S(x)(y-x) < \int_x^yS(t)\dd{t}
            = C(x)-C(y)
            \leq 2
        \end{equation*}
        \item But we can choose $y$ large enough to make $S(x)(y-x)>2$, a contradiction.
    \end{itemize}
    \item $\bm{\pi}$: The real number such that $\pi/2$ is the unique smallest positive real number with $C(\pi/2)=0$.
    \begin{itemize}
        \item We know that a unique smallest number exists because since $C(0)=1$ and $C$ is continuous, there exists a neighborhood around 0 where $C$ is nonzero.
    \end{itemize}
    \item Proposition: $S(\pi/2)=1$.
    \begin{itemize}
        \item We have
        \begin{align*}
            C(\pi/2)^2+S(\pi/2)^2 &= 1\\
            S(\pi/2) &= \pm 1
        \end{align*}
        \item Furthermore, since $S(0)=0$ and $S'(x)=C(x)$ is positive on $[0,\pi/2)$, we know that $S$ is increasing and thus $S(\pi/2)=+1$.
    \end{itemize}
\end{itemize}




\end{document}