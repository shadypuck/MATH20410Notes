\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{7}

\begin{document}




\chapter{Some Special Functions}
\section{Notes}
\begin{itemize}
    \item \marginnote{3/7:}Plan:
    \begin{enumerate}
        \item Go over some of the hits in chapter 8.
        \item Define sine.
        \item Power series.
        \item Exponential functions ($\log$, $\sin$, $\cos$).
    \end{enumerate}
    \item Proposition (power series properties): If $\sum_{n=0}^\infty a_nx^n$ converges for all $|x|<R$, and $f:B_R(0)\to\R$ is defined by
    \begin{equation*}
        f(x) = \sum_{n=0}^\infty a_nx^n
    \end{equation*}
    then:
    \begin{enumerate}[label={(\alph*)}]
        \item $f$ is continuous.
        \begin{itemize}
            \item From the root test, $\sum_{n=0}^\infty a_nx^n$ is in fact absolutely convergent on $(-R,R)$. Therefore, on any interval $[-R+\epsilon,R-\epsilon]$ ($0<\epsilon<R$), we have
            \begin{equation*}
                |a_nx^n| \leq |a_n||R+\epsilon|^n
            \end{equation*}
            so by the $M$-test, $\sum_{n=0}^\infty a_nx^n$ converges uniformly on $[-R+\epsilon,R-\epsilon]$. Then since $\sum_{n=0}^\infty a_nx^n$ converges uniformly on $[-R+\epsilon,R-\epsilon]$, we have (a) since all $\sum_{n=0}^Na_nx^n$ are continuous.
        \end{itemize}
        \item $f$ is differentiable with $f'(x)=\sum_{n=1}^\infty na_nx^{n-1}$.
        \begin{itemize}
            \item (b) follows similarly to (a) by uniform convergence.
            \item Note that $\limsup\sqrt[n]{|na_n|}=\limsup\sqrt[n]{|a_n|}$ (since $\lim_{n\to\infty}\sqrt[n]{n}=1$).
            \item Therefore, $\sum_{n=1}^\infty na_nx^{n-1}$ converges on $(-R,R)$.
        \end{itemize}
        \item More generally, $f$ is infinitely differentiable with
        \begin{equation*}
            f^{(k)}(x) = \sum_{n=k}^\infty\frac{n!}{(n-k)!}a_nx^{n-k}
        \end{equation*}
        \begin{itemize}
            \item Now (c) follows as in the proof of (b).
        \end{itemize}
        \item We have the identity
        \begin{equation*}
            a_k = \frac{f^{(k)}(0)}{k!}
        \end{equation*}
        \begin{itemize}
            \item (d) follows from (c) by plugging in zero.
        \end{itemize}
    \end{enumerate}
    \item Note that historically, the analysis of power series motivated the development of all of the Chapter 7 theorems; we simply learned those first without motivation to present the proofs in an ordered manner.
    \item Aside: Consider the exponential function $x^y$ for $x,y\in\R$ with $x\geq 0$.
    \begin{itemize}
        \item We define it for natural numbers and integers fairly easily, then rationals, and then for reals as the supremum of exponentials of the entries in the Dedekind cut below $x\in\R$.
        \item Under this definition, we can confirm our normal exponential rules and then that $x^y$ is continuous.
    \end{itemize}
    \item Recall that
    \begin{equation*}
        \e = \sum_{n=0}^\infty\frac{1}{n!}
    \end{equation*}
    \item So now we are going to construct $E(x)$, $L(x)$, $C(x)$, and $S(x)$ (which are just $\e[x]$, $\ln(x)$, $\cos(x)$, and $\sin(x)$).
    \item Define $E:\C\to\C$ by
    \begin{equation*}
        E(z) = \sum_{n=0}^\infty\frac{z^n}{n!}
    \end{equation*}
    \begin{itemize}
        \item By the proposition, it converges and is continuous for all $z\in\C$.
        \item For the real numbers, $E$ is differentiable. ($E$ is also complex-differentiable, but we won't go into that).
    \end{itemize}
    \item Proposition: $E(z)E(w)=E(z+w)$ for all $z,w\in\C$.
    \begin{itemize}
        \item We have by the Cauchy product (Mertens' theorem) that
        \begin{align*}
            E(z)E(w) &= \left( \sum_{n=0}^\infty\frac{z^n}{n!} \right)\left( \sum_{n=0}^\infty\frac{w^n}{n!} \right)\\
            &= \sum_{n=0}^\infty\sum_{k=0}^n\frac{z^kw^{n-k}}{k!(n-k)!}\\
            &= \sum_{n=0}^\infty\frac{1}{n!}\sum_{k=0}^n\binom{n}{k}z^kw^{n-k}\\
            &= \sum_{n=0}^\infty\frac{1}{n!}(z+w)^n\\
            &= E(z+w)
        \end{align*}
    \end{itemize}
    \item Corollary: $E(z)E(-z)=E(0)=1$ for all $z\in\C$.
    \item $E(x)>0$ for $x\geq 0$.
    \begin{itemize}
        \item It follows since $E(z+w)=E(z)E(w)$ that $E(x)>0$ for all $x\in\R$.
    \end{itemize}
    \item $\dv*{E}{x}=E$; $E$ is the unique, normalized ($E(0)=1$) function such that this is true.
    \begin{itemize}
        \item We can prove this from the power series definition.
    \end{itemize}
    \item $E(x)\to\infty$ as $x\to\infty$ and $E(x)\to 0$ as $x\to -\infty$. (Also from the power series definition.)
    \item $0\leq x_1<x_2$ implies that $E(x_1)<E(x_2)$.
    \begin{itemize}
        \item Either from $\dv*{E}{x}=E>0$ or from the power series definition.
        \item It follows from $E(z+w)=E(z)E(w)$ that $x_1<x_2$ implies $E(x_1)<E(x_2)$.
    \end{itemize}
    \item \marginnote{3/9:}Plan:
    \begin{enumerate}
        \item Keep going with $E$, $L$, $C$, and $S$.
        \item Prove the fundamental theorem of algebra.
    \end{enumerate}
    \item Define
    \begin{equation*}
        E(z) = \sum_{n=0}^\infty\frac{z^n}{n!}
    \end{equation*}
    \begin{itemize}
        \item Recall that $E(z+w)=E(z)E(w)$.
    \end{itemize}
    \item Theorem: $E(x)=\e[x]$ for all $x\in\R$.
    \begin{itemize}
        \item $E(1)=\e[1]$ (by definition).
        \item $E(n)=\e[n]$ (by $E(z+w)=E(z)E(w)$).
        \item $[E(p/q)]^q=E(p)=\e[p]$ (by $E(z+w)=E(z)E(w)$).
        \item $E(p/q)=\e[p/q]$ for all $p/q\in\Q$.
        \item $E(x)=\e[x]$ for all $x\in\R$ (since both LHS and RHS are continuous functions that agree on $\Q$).
    \end{itemize}
    \item Briefly: $E:\R\to\R^+$ is a strictly increasing surjective function. Thus, we have an inverse function $L:\R^+\to\R$.
    \item Theorem: $L$ is differentiable (and therefore continuous).
    \begin{itemize}
        \item Since $E'=E>0$ everywhere, we may apply the inverse function theorem at every point.
    \end{itemize}
    \item Now by the chain rule, $E(L(x))=x$ for all $x\in\R^+$, so taking derivatives yields
    \begin{align*}
        E'(L(x))L'(x) &= 1\\
        E(L(x))L'(x) &= 1\\
        xL'(x) &= 1\\
        L'(x) &= \frac{1}{x}
    \end{align*}
    \item Proposition:
    \begin{enumerate}
        \item $L(uw)=L(u)+L(w)$.
        \item $L(x)=\int_1^xt^{-1}\dd{t}$.
    \end{enumerate}
    \item Trig functions:
    \begin{align*}
        C(x) &= \frac{1}{2}[E(ix)+E(-ix)]&
        S(x) &= \frac{1}{2i}[E(ix)-E(-ix)]
    \end{align*}
    \begin{itemize}
        \item You can use these definitions to prove trig identities, having derived them geometrically.
    \end{itemize}
    \item Proposition: If $x\in\R$, then $C(x),S(x)\in\R$.
    \begin{itemize}
        \item Key observation: $E(\bar{z})=\overline{E(z)}$.
        \item We have
        \begin{align*}
            \overline{C(x)} &= \frac{1}{2}[\overline{E(ix)}+\overline{E(-ix)}]\\
            &= \frac{1}{2}[E(-ix)+E(ix)]\\
            &= C(x)
        \end{align*}
        \item Symmetric for $S(x)$.
    \end{itemize}
    \item Note that we could equally well define $C,S$ by
    \begin{align*}
        C(x) &= \sum_{k=0}^\infty\frac{(-1)^k}{(2k)!}x^{2k}&
        S(x) &= \sum_{k=0}^\infty\frac{(-1)^k}{(2k+1)!}x^{2k+1}
    \end{align*}
    \item Proposition: $E(ix)=C(x)+iS(x)$.
    \item Proposition: $C,S$ are differentiable with
    \begin{align*}
        C'(x) &= -S(x)&
        S'(x) &= C(x)
    \end{align*}
    \item Proposition: For all $x\in\R$, $|E(ix)|=1$.
    \begin{itemize}
        \item We have that
        \begin{equation*}
            |E(ix)|^2 = E(ix)\overline{E(ix)}
            = E(ix)E(-ix)
            = E(0)
            = 1
        \end{equation*}
        \item Taking square roots of both sides yields the desired result.
    \end{itemize}
    \item The above result proves that the imaginary axis maps onto the unit circle in the complex plane.
    \item We now define $\pi$ and all that.
    \begin{itemize}
        \item Goal: Show that for all $z\in\C$ with $|z|=1$, there exists a unique $\theta\in[0,2\pi)$ such that $\e[i\theta]=z$. Further, $E(ix)$ has period $2\pi$.
    \end{itemize}
    \item Proposition: $C(x)^2+S(x)^2=1$.
    \begin{itemize}
        \item Use $E(ix)=C(x)+iS(x)$ and $|E(ix)|=1$.
    \end{itemize}
    \item Proposition: There exists some positive number $x$ such that $C(x)=0$.
    \begin{itemize}
        \item Suppose (contradiction): $C(x)>0$ for all $x>0$ (since $C(0)=1$).
        \item Thus, $S'(x)>0$ for all $x>0$.
        \item Consequently, given $0<x<y$,
        \begin{equation*}
            S(x)(y-x) < \int_x^yS(t)\dd{t}
            = C(x)-C(y)
            \leq 2
        \end{equation*}
        \item But we can choose $y$ large enough to make $S(x)(y-x)>2$, a contradiction.
    \end{itemize}
    \item $\bm{\pi}$: The real number such that $\pi/2$ is the unique smallest positive real number with $C(\pi/2)=0$.
    \begin{itemize}
        \item We know that a unique smallest number exists because since $C(0)=1$ and $C$ is continuous, there exists a neighborhood around 0 where $C$ is nonzero.
    \end{itemize}
    \item Proposition: $S(\pi/2)=1$.
    \begin{itemize}
        \item We have
        \begin{align*}
            C(\pi/2)^2+S(\pi/2)^2 &= 1\\
            S(\pi/2) &= \pm 1
        \end{align*}
        \item Furthermore, since $S(0)=0$ and $S'(x)=C(x)$ is positive on $[0,\pi/2)$, we know that $S$ is increasing and thus $S(\pi/2)=+1$.
    \end{itemize}
\end{itemize}



\section{Chapter 8: Some Special Functions}
\begin{itemize}
    \item \marginnote{3/10:}\textbf{Analytic function}: A function that can be represented by a power series.
    \item Theorem 8.1: If
    \begin{equation*}
        f(x) = \sum_{n=0}^\infty c_nx^n
    \end{equation*}
    converges for $|x|<R$, then\dots
    \begin{enumerate}
        \item $f$ converges uniformly on $[-R+\epsilon,R-\epsilon]$ for all $\epsilon>0$;
        \item $f$ is continuous and differentiable on $(-R,R)$;
        \item We have the identity
        \begin{equation*}
            f'(x) = \sum_{n=1}^\infty nc_nx^{n-1}
        \end{equation*}
        for all $|x|<R$.
    \end{enumerate}
    \item Corollary: If $f$ satisfies the hypotheses of Theorem 8.1, then $f$ has derivatives of all orders in $(-R,R)$ given by
    \begin{equation*}
        f^{(k)}(x) = \sum_{n=k}^\infty\frac{n!}{(n-k)!}c_nx^{n-k}
    \end{equation*}
    In particular,
    \begin{equation*}
        f^{(k)}(0) = k!c_k
    \end{equation*}
    for all $k\in\N_0$.
    \item Note that there exist functions $f$ that have derivatives of all orders at a point but cannot be expanded in a power series at that point (see Exercise 8.1).
    \item Theorem 8.2: If $\sum c_n$ converges and
    \begin{equation*}
        f(x) = \sum_{n=0}^\infty c_nx^n
    \end{equation*}
    for $|x|<1$, then
    \begin{equation*}
        \lim_{x\to 1}f(x) = \sum_{n=0}^\infty c_n
    \end{equation*}
    \item Theorem 8.3: If $\{a_{ij}\}$ ($i,j\in\N$) is a double sequence, $\{b_i\}$ is defined by
    \begin{equation*}
        b_i = \sum_{j=1}^\infty|a_{ij}|
    \end{equation*}
    for all $i\in\N$, and $\sum b_i$ converges, then
    \begin{equation*}
        \sum_{i=1}^\infty\sum_{j=1}^\infty a_{ij} = \sum_{j=1}^\infty\sum_{i=1}^\infty a_{ij}
    \end{equation*}
    \item Theorem 8.4: If
    \begin{equation*}
        f(x) = \sum_{n=0}^\infty c_nx^n
    \end{equation*}
    converges for $|x|<R$ and $a\in(-R,R)$, then $f$ can be expanded in a power series about $x=a$ which converges in $|x-a|<R-|a|$ and
    \begin{equation*}
        f(x) = \sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n
    \end{equation*}
    for all $|x-a|<R-|a|$.
    \begin{itemize}
        \item "This is an extension of Theorem 5.15 and is als known as Taylor's theorem" \parencite[176]{bib:Rudin}.
    \end{itemize}
    \item Theorem 8.5: If $\sum a_nx^n,\sum b_nx^n$ converge on $S=(-R,R)$, $E$ is the set of all $x\in S$ at which
    \begin{equation*}
        \sum_{n=0}^\infty a_nx^n = \sum_{n=0}^\infty b_nx^n
    \end{equation*}
    and $E$ has a limit point in $S$, then $a_n=b_n$ for $n\in\N_0$. Hence, the above equation holds for all $x\in S$.
    \item $\bm{E}$: The function defined as follows for all $z\in\C$. \emph{Given by}
    \begin{equation*}
        E(z) = \sum_{n=0}^\infty\frac{z^n}{n!}
    \end{equation*}
    \item We have that $E(z+w)=E(z)E(w)$ and thus $E(z)E(-z)=E(z-z)=E(0)=1$ for all $z,w\in\C$.
    \item Thus, $E(x)=1/E(-x)>0$ for all $x\in\R$.
    \item It follows since $E(x)\to\infty$ as $x\to\infty$ that $E(x)\to 0$ as $x\to -\infty$.
    \item $0<x<y$ implies $E(x)<E(y)$.
    \item We have that
    \begin{equation*}
        E'(z) = \lim_{h\to 0}\frac{E(z+h)-E(z)}{h}
        = E(z)\lim_{h\to 0}\frac{E(h)-1}{h}
        = E(z)
    \end{equation*}
    \item \textcite{bib:Rudin} proves that $E(x)=\e[x]$ for all $x\in\R$ as in class.
    \item Theorem 8.6: Let $\e[x]$ be defined on $\R$ as above. Then
    \begin{enumerate}[label={(\alph*)}]
        \item $\e[x]$ is continuous and differentiable for all $x$.
        \item $(\e[x])'=\e[x]$.
        \item $\e[x]$ is a strictly increasing function of $x$, and $\e[x]>0$.
        \item $\e[x+y]=\e[x]\e[y]$.
        \item $\e[x]\to\infty$ as $x\to\infty$ and $\e[x]\to 0$ as $x\to -\infty$.
        \item $\lim_{x\to\infty}x^n\e[-x]=0$ for all $n$.
    \end{enumerate}
    \item Theorem 8.6f shows that $\e[x]$ tends to infinity faster than any power of $x$.
    \item $\bm{L}$: The inverse of $E$, implied to exist by the IVT since $E$ is strictly increasing and differentiable on $\R$.
    \item Differentiating $L(E(x))=x$ with the chain rule reveals that $L'(y)=1/y$.
    \item Since $L(1)=L(E(0))=0$, the FTC implies that $L(y)=\int_1^y\dd{x}/x$.
    \item If $E(x)=u$ and $E(y)=v$, then
    \begin{align*}
        L(uv) &= L(E(x)E(y))\\
        &= L(E(x+y))\\
        &= x+y\\
        &= L(u)+L(v)
    \end{align*}
    \item We define $x^n=E(nL(x))$ for all $x>0$ and $n\in\N$, which we can extend analogously to before to $x^y$ for any $x>0$ and $y\in\R$.
    \item In the same vein, we have that
    \begin{equation*}
        (x^\alpha)' = E(\alpha L(x))\cdot\frac{\alpha}{x}
        = \alpha x^{\alpha-1}
    \end{equation*}
    \item We also have $\lim_{x\to\infty}x^{-\alpha}\log x=0$, i.e., that $\log x\to\infty$ slower than any positive power of $x$.
    \item We define
    \begin{align*}
        C(x) &= \frac{1}{2}[E(ix)+E(-ix)]&
        S(x) &= \frac{1}{2i}[E(ix)-E(-ix)]
    \end{align*}
    \item We know that $E(\bar{z})=\overline{E(z)}$, so $C(x),S(x)$ are real for real $x$.
    \item Also, $E(ix)=C(x)+iS(x)$.
    \item We have $|E(ix)|=1$ for all $x\in\R$.
    \item We have $C(0)=1$ and $S(0)=0$.
    \item We have
    \begin{align*}
        C'(x) &= -S(x)&
        S'(x) &= C(x)
    \end{align*}
    \item \textcite{bib:Rudin} proves, as in class, that there exist positive numbers $x$ for which $C(x)=0$.
    \item A smallest positive number such that $C(x)=0$ exists since $f^{-1}(\{0\})$ is closed as the preimage of a closed set under a continuous function.
    \item We can prove as in class that $C(\pi/2)=0$ and $S(\pi/2)=1$. It follows that
    \begin{equation*}
        E(i\frac{\pi}{2}) = i
    \end{equation*}
    so that, by the addition formula, $E(2\pi i)=1$, and hence $E(z+2\pi i)=E(z)$ by the addition formula for all $z\in\C$.
    \item Theorem 8.7:
    \begin{enumerate}[label={(\alph*)}]
        \item $E$ is periodic with period $2\pi i$.
        \item $C,S$ are periodic with period $2\pi$.
        \item $0<t<2\pi$ implies that $E(it)\neq 1$.
        \item $z\in\C$ with $|z|=1$ implies there is a unique $t\in[0,2\pi)$ with $E(it)=z$.
    \end{enumerate}
    \item Calculating the circumference of a circle.
    \begin{itemize}
        \item Consider the curve $\gamma:[0,2\pi]\to\C$ defined by
        \begin{equation*}
            \gamma(t) = E(it)
        \end{equation*}
        \item This is a simple closed curve in the plane whose range is exactly the unit circle in the plane.
        \item Thus, since $\gamma'(t)=iE(it)$, the length of $\gamma$ (i.e., the circumference of the unit circle) is
        \begin{equation*}
            \int_0^{2\pi}|\gamma'(t)|\dd{t} = 2\pi
        \end{equation*}
        \item This shows that $\pi$ has the same geometric significance in analysis with which it was originally defined in geometry.
    \end{itemize}
    \item Similarly, we can consider the triangle with vertices at $z_1=0$, $z_2=\gamma(t_0)$, and $z_3=C(t_0)$ to recover the original geometric definition of $C(t)$.
    \begin{itemize}
        \item We can do the same with $S$.
    \end{itemize}
\end{itemize}




\end{document}