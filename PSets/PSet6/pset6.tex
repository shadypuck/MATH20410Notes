\documentclass[../psets.tex]{subfiles}

\pagestyle{main}
\renewcommand{\leftmark}{Problem Set \thesection}
\setcounter{section}{5}

\begin{document}




\section{Functions of Several Variables II}
\emph{From \textcite{bib:Rudin}.}
\subsection*{Chapter 9}
\begin{enumerate}[label={\textbf{\arabic*.}}]
    \setcounter{enumi}{4}
    \item \marginnote{2/22:}Prove that to every $A\in L(\R^n,\R^1)$ corresponds a unique $\vec{y}\in\R^n$ such that $A\vec{x}=\vec{x}\cdot\vec{y}$. Prove also that $\norm{A}=|\vec{y}|$. (Hint: Under certain conditions, equality holds in the Schwarz inequality.)
    \begin{proof}
        Let $\vec{y}$ be defined as follows.
        \begin{equation*}
            \vec{y} =
            \begin{bmatrix}
                A\vec{e}_1\\
                \vdots\\
                A\vec{e}_n\\
            \end{bmatrix}
        \end{equation*}
        Then if $\vec{x}=a_1\vec{e}_1+\cdots+a_n\vec{e}_n$, we have that
        \begin{equation*}
            A\vec{x} = a_1\cdot A\vec{e}_1+\cdots+a_n\cdot A\vec{e}_n
            = \vec{x}\cdot\vec{y}
        \end{equation*}
        as desired.\par
        Now suppose that $\vec{z}$ satisfies $A\vec{x}=\vec{x}\cdot\vec{z}$ for all $\vec{x}\in\R^n$. Then
        \begin{align*}
            \vec{x}\cdot\vec{y} &= \vec{x}\cdot\vec{z}\\
            \vec{x}\cdot(\vec{y}-\vec{z}) &= 0
        \end{align*}
        for all $\vec{x}\in\R^n$. In particular, if $\vec{x}=\vec{y}-\vec{z}$, then
        \begin{align*}
            0 &= (\vec{y}-\vec{z})\cdot(\vec{y}-\vec{z})\\
            &= \norm{\vec{y}-\vec{z}}^2\\
            \bm{0} &= \vec{y}-\vec{z}\\
            \vec{z} &= \vec{y}
        \end{align*}
        as desired.\par
        We also have by the Cauchy-Schwarz inequality that
        \begin{equation*}
            \norm{A} = \sup_{\substack{\vec{x}\in\R^n\\|\vec{x}|=1}}|A\vec{x}|
            = \sup_{\substack{\vec{x}\in\R^n\\|\vec{x}|=1}}|\vec{x}\cdot\vec{y}|
            \leq \sup_{\substack{\vec{x}\in\R^n\\|\vec{x}|=1}}|\vec{x}|\cdot|\vec{y}|
            = |\vec{y}|
        \end{equation*}
        Moreover, equality holds by noting that if $\vec{x}=\vec{y}/|\vec{y}|$, then
        \begin{equation*}
            |\vec{x}\cdot\vec{y}| = \frac{|\vec{y}|^2}{|\vec{y}|} = |\vec{y}|
        \end{equation*}
        so the leftmost supremum is also at least $|\vec{y}|$, as desired.
    \end{proof}
    \item If
    \begin{equation*}
        f(x,y) =
        \begin{cases}
            0 & (x,y)=(0,0)\\
            \frac{xy}{x^2+y^2} & (x,y)\neq(0,0)
        \end{cases}
    \end{equation*}
    prove that $(D_1f)(x,y)$ and $(D_2f)(x,y)$ exist at every point of $\R^2$, although $f$ is not continuous at $(0,0)$.
    \begin{proof}
        Let $(x,y)\in\R^2$ be arbitrary. We divide into two cases ($(x,y)\neq(0,0)$ and $(x,y)=(0,0)$). If $(x,y)\neq(0,0)$, then as the sum, product, and quotient of linear (hence differentiable) functions, $f$ is differentiable at $(x,y)$. Therefore, by Theorem 9.17, $(D_1f)(x,y)$ and $(D_2f)(x,y)$ exist. On the other hand, if $(x,y)=(0,0)$, then
        \begin{align*}
            (D_jf)(0,0) &= \lim_{t\to 0}\frac{f(t\vec{e}_j)-f(0,0)}{t}\\
            &= \lim_{t\to 0}\frac{\frac{t\cdot 0}{t^2+0^2}-0}{t}\\
            &= \lim_{t\to 0}0\\
            &= 0
        \end{align*}
        However, since
        \begin{equation*}
            f(t,t) = \frac{t^2}{t^2+t^2} = \frac{1}{2}
        \end{equation*}
        for all $t$ (notably arbitrarily small $t$), we find points $(x,y)\in\R^2$ arbitrarily close to $(0,0)$ for which $f(x,y)=1/2$. Therefore, $f$ is not continuous at $(0,0)$.
    \end{proof}
    \item Suppose that $f$ is a real-valued function defined in an open set $E\subset\R^n$, and that the partial derivatives $D_1f,\dots,D_nf$ are bounded on $E$. Prove that $f$ is continuous in $E$. (Hint: Proceed as in the proof of Theorem 9.21.)
    \begin{proof}
        To prove that $f$ is continuous at an arbitrary $\vec{x}\in E$, it will suffice to show that for all $\epsilon>0$, there exists a $\delta>0$ such that if $\vec{x}+\vec{h}\in E$ and $|\vec{h}|<\delta$, then $|f(\vec{x}+\vec{h})-f(\vec{x})|<\epsilon$. Let $\epsilon>0$ be arbitrary. Since each $D_jf$ is bounded, we know that there exist $M_1,\dots,M_n$ such that $|D_jf|\leq M_j$ ($j=1,\dots,n$). Define $M=\max M_j$. Choose $\delta=\epsilon/nM$. Let $\vec{h}$ be such that $\vec{x}+\vec{h}\in E$ and $|\vec{h}|<\delta$. Supposing $\vec{h}=\sum h_j\vec{e}_j$, put $\vec{v}_0=\bm{0}$ and $\vec{v}_k=h_1\vec{e}_1+\cdots+h_k\vec{e}_k$ ($k=1,\dots,n$). Then, letting $\theta_j\in(0,1)$ for all $j$, we have that
        \begin{align*}
            |f(\vec{x}+\vec{h})-f(\vec{x})| &= \left| \sum_{j=1}^n[f(\vec{x}+\vec{v}_j)-f(\vec{x}+\vec{v}_{j-1})] \right|\\
            &\leq \sum_{j=1}^n|f(\vec{x}+\vec{v}_j)-f(\vec{x}+\vec{v}_{j-1})|\\
            &= \sum_{j=1}^n[h_j(D_jf)(\vec{x}+\vec{v}_{j-1}+\theta_jh_j\vec{e}_j)]\tag*{MVT}\\
            &\leq M\sum_{j=1}^nh_j\\
            &\leq nM|\vec{h}|\\
            &< \epsilon
        \end{align*}
        as desired.
    \end{proof}
    \item Suppose that $f$ is a differentiable real function in an open set $E\subset\R^n$, and that $f$ has a local maximum at a point $\vec{x}\in E$. Prove that $f'(\vec{x})=0$.
    \begin{proof}
        To prove that $f'(\vec{x})=0$, Theorem 9.17 tells us that it will suffice to show that $(D_jf)(\vec{x})=0$ for all $1\leq j\leq n$. Let $j$ be an arbitrary natural number between 1 and $n$, inclusive. Define $U_j=(\vec{x}+\spn(\vec{e}_j))\cap E$ and let $\vec{x}=(x_1,\dots,x_n)$. Then by definition, $(D_jf)(\vec{x})=(f|_{U_j})'(x_j)$ and $f|_{U_j}(x_j)$ is a local maximum of $f|_{U_j}$. It follows by Theorem 5.8 that
        \begin{equation*}
            (D_jf)(\vec{x}) = (f|_{U_j})'(x_j) = 0
        \end{equation*}
        as desired.
    \end{proof}
    \stepcounter{enumi}
    \item If $f$ is a real function defined in a convex open set $E\subset\R^n$, such that $(D_1f)(\vec{x})=0$ for every $\vec{x}\in E$, prove that $f(\vec{x})$ depends only on $x_2,\dots,x_n$. Show that the convexity of $E$ can be replaced by a weaker condition, but that some condition is required. For example, if $n=2$ and $E$ is shaped like a horseshoe, the statement may be false.
    \begin{proof}
        To prove that $f(\vec{x})$ depends only on $x_2,\dots,x_n$, it will suffice to show that if $\vec{x},\vec{y}\in E$ are such that $\vec{x}=(x,x_2,\dots,x_n)$ and $\vec{y}=(y,x_2,\dots,x_n)$, then $f(\vec{x})=f(\vec{y})$. Let $\vec{x},\vec{y}\in E$ be arbitrary points that satisfy the previous condition. Since $E$ is convex, $\lambda\vec{x}+(1-\lambda)\vec{y}\in E$ for all $\lambda\in(0,1)$. Define $U=\{\lambda\vec{x}+(1-\lambda)\vec{y}:0\leq\lambda\leq 1\}$. Then since $(f|_U)'(t_1)=(D_1f)(\vec{t})=0$ on $U$, Theorem 5.17b implies that $f|_U$ is constant. In particular, since $\vec{x},\vec{y}\in U$, $f(\vec{x})=f(\vec{y})$, as desired.\par
        The following weaker condition will suffice: $E$ open is a set such that $\lambda\vec{x}+(1-\lambda)\vec{y}\in E$ for all $0<\lambda<1$ and $\vec{x},\vec{y}\in E$ \emph{that satisfy} $\vec{x}-\vec{y}\in\spn(\vec{e}_1)$. Note that the above proof still works with this condition in place of convexity because the arbitrary $\vec{x},\vec{y}$ to which we applied the definition of convexity satisfy $\vec{x}-\vec{y}\in\spn(\vec{e}_1)$.\par
        If we do not assert any condition, we may consider as a counterexample the subset of $\R^2$
        \begin{equation*}
            E = ((-2,-1)\times(1,2))\cup((1,2)\times(1,2))
        \end{equation*}
        and the function $f:E\to\R$ defined by
        \begin{equation*}
            f(x,y) = \sgn(x)+y
        \end{equation*}
        Since $f(x,y)=-1+y$ for $x\in(-2,-1)\times(1,2)$ and $f(x,y)=1+y$ for $x\in(1,2)\times(1,2)$, clearly $(D_1f)(\vec{x})=0$ for all $\vec{x}\in E$. Yet $f(-1.5,0)=-1$ and $f(1.5,0)=1$.
    \end{proof}
    \item If $f$ and $g$ are differentiable real functions in $\R^n$, prove that
    \begin{equation*}
        \nabla(fg) = f\nabla g+g\nabla f
    \end{equation*}
    and that
    \begin{equation*}
        \nabla\left( \frac{1}{f} \right) = -\frac{\nabla f}{f^2}
    \end{equation*}
    wherever $f\neq 0$.
    \begin{proof}
        Since, by definition, partial derivatives only take limits over linear subsets of $\R^n$, i.e., ones that are isomorphic to $\R$, they behave like the one-dimensional derivatives of $f$ restricted to such linear subsets. More precisely, $D_jf$ behaves like the derivative of $(f|_{N_r(\vec{x})\cap\spn(\vec{e}_j)})'$ where $\vec{x}$ is the point at which we're taking the derivative and $r$ is taken to be small enough so that $N_r(\vec{x})$ is a subset of the open domain of $f$. In particular, this means that partial derivatives obey the usual sum, product, and quotient rules, as well as other rules carried over from the analysis of functions of a single variable. Thus, we have that for any $\vec{x}\in\R^n$,
        \begin{align*}
            (\nabla(fg))(\vec{x}) &= \sum_{j=1}^n(D_j(fg))(\vec{x})\vec{e}_j\\
            &= \sum_{j=1}^n(fD_jg+D_jfg)(\vec{x})\vec{e}_j\\
            &= f(\vec{x})\sum_{j=1}^n(D_jg)(\vec{x})\vec{e}_j+g(\vec{x})\sum_{j=1}^n(D_jf)(\vec{x})\vec{e}_j\\
            &= f(\vec{x})(\nabla g)(\vec{x})+(\nabla f)(\vec{x})g(\vec{x})\\
            &= (f\nabla g+g\nabla f)(\vec{x})
        \end{align*}
        and symmetrically for $\nabla(1/f)$.
    \end{proof}
    \setcounter{enumi}{16}
    \item Let $\mathbf{f}=(f_1,f_2)$ be the mapping of $\R^2$ into $\R^2$ given by
    \begin{align*}
        f_1(x,y) &= \e[x]\cos y&
        f_2(x,y) &= \e[x]\sin y
    \end{align*}
    \begin{enumerate}
        \item What is the range of $f$?
        \begin{proof}
            The range of $f$ is $\R^2\setminus\{\bm{0}\}$.\par
            Both trigonometric functions have range $[-1,1]$ and $\e[x]$ has range $(0,\infty)$, so we can make $\e[x]$ arbitrarily large and then take $y$ such that such that the trigonometric function equals 1 or $-1$. However, since $\cos y=\pm 1$ when $\sin y=0$ and vice versa, we can never achieve $(0,0)$.
        \end{proof}
        \item Show that the Jacobian of $f$ is not zero at any point of $\R^2$. Thus, every point of $\R^2$ has a neighborhood in which $f$ is one-to-one. Nevertheless, $f$ is not one-to-one on $\R^2$.
        \begin{proof}
            Let $(x,y)\in\R^2$ be arbitrary. We divide into two cases ($y=n\pi$ and $y\neq n\pi$ [$n\in\Z$]). If $y=n\pi$, then since $(\sin y)'=\cos y$, $D_2f_2=\e[x]\cos y$, $\e[x]\neq 0$ for all $x$, and $\cos n\pi=\pm 1$ ($n\in\Z$), we have that $(D_2f_2)(x,y)\neq 0$. If $y\neq n\pi$, then since $(\e[x])'=\e[x]$ (Theorem 8.6b), $D_1f_2=\e[x]\sin y$, $\e[x]\neq 0$ for all $x$, and $\sin y\neq 0$ for all $y\neq n\pi$, we have that $(D_1f_2)(x,y)\neq 0$.\par
            It follows by Theorem 9.24 that every point of $\R^2$ has a neighborhood in which $f$ is 1-1.\par
            Since the trigonometric functions are periodic with period $2\pi$, $f(0,0)=f(0,2\pi)$, for instance.
        \end{proof}
        \item Put $\vec{a}=(0,\pi/3)$, $\vec{b}=f(\vec{a})$, and let $\mathbf{g}$ be the continuous inverse of $\mathbf{f}$, defined in a neighborhood of $\vec{b}$, such that $\mathbf{g}(\vec{b})=\vec{a}$. Find an explicit formula for $\mathbf{g}$, compute $\mathbf{f}'(\vec{a})$ and $\mathbf{g}'(\vec{b})$, and verify that
        \begin{equation*}
            \mathbf{g}'(\vec{b}) = [\mathbf{f}'(\mathbf{g}(\vec{b}))]^{-1}
        \end{equation*}
        \begin{proof}
            Let $U=\R\times(0,\pi/2)$. Then $V=f(U)=(0,\infty)^2$. Thus, we may define $\mathbf{g}:V\to U$ by
            \begin{align*}
                g_1(x,y) &= \frac{1}{2}\ln(x^2+y^2)&
                g_2(x,y) &= \tan^{-1}\left( \frac{y}{x} \right)
            \end{align*}
            Under this definition, we can easily see that if $(x,y)\in U$, then
            \begin{align*}
                g(f(x,y)) &= g(\e[x]\cos y,\e[x]\sin y)\\
                &= \left( \frac{1}{2}\ln\left( (\e[x]\cos y)^2+(\e[x]\sin y)^2 \right),\tan^{-1}\left( \frac{\e[x]\sin y}{\e[x]\cos y} \right) \right)\\
                &= \left( \frac{1}{2}\ln\left( \e[2x](\cos^2y+\sin^2y) \right),\tan^{-1}(\tan y) \right)\\
                &= (x,y)
            \end{align*}
            as desired. We can also compute that
            \begin{align*}
                \mathbf{f}'(\vec{a}) &=
                \begin{bNiceMatrix}
                    (D_1f_1)(\vec{a}) & (D_2f_1)(\vec{a})\\
                    (D_1f_2)(\vec{a}) & (D_2f_2)(\vec{a})\\
                \end{bNiceMatrix}&
                    \mathbf{g}'(\vec{b}) &=
                    \begin{bNiceMatrix}
                        (D_1f_1)(\vec{a}) & (D_2f_1)(\vec{a})\\
                        (D_1f_2)(\vec{a}) & (D_2f_2)(\vec{a})\\
                    \end{bNiceMatrix}\\
                &=
                \begin{bNiceMatrix}
                    \e[x]\cos y & -\e[x]\sin y\\
                    \e[x]\sin y & \e[x]\cos y\\
                \end{bNiceMatrix}&
                    &=
                    \begin{bNiceMatrix}
                        \frac{x}{x^2+y^2} & \frac{y}{x^2+y^2}\\
                        -\frac{y}{x^2+y^2} & \frac{x}{x^2+y^2}\\
                    \end{bNiceMatrix}\\
                &=
                \begin{bNiceMatrix}
                    \frac{1}{2} & -\frac{\sqrt{3}}{2}\\
                    \frac{\sqrt{3}}{2} & \frac{1}{2}\\
                \end{bNiceMatrix}&
                    &=
                    \begin{bNiceMatrix}
                        \frac{1}{2} & \frac{\sqrt{3}}{2}\\
                        -\frac{\sqrt{3}}{2} & \frac{1}{2}
                    \end{bNiceMatrix}
            \end{align*}
            It follows from the definition of matrix multiplication that
            \begin{align*}
                \mathbf{g}'(\vec{b})\mathbf{f}'(\mathbf{g}(\vec{b})) &= \mathbf{g}'(\vec{b})\mathbf{f}'(\vec{a}) = I&
                \mathbf{f}'(\mathbf{g}(\vec{b}))\mathbf{g}'(\vec{b}) &= \mathbf{f}'(\vec{a})\mathbf{g}'(\vec{b}) = I
            \end{align*}
            as desired.
        \end{proof}
        \item What are the images under $\mathbf{f}$ of lines parallel to the coordinate axes?
        \begin{proof}
            Let $U\subset\R^2$ denote a line parallel to the $x$-axis and a distance $t$ away, and let $V\subset\R^2$ denote a line parallel to the $y$-axis and a distance $t$ away. Formally,
            \begin{align*}
                U &= \{(t,y):y\in\R\}&
                V &= \{(x,t):x\in\R\}
            \end{align*}
            where $t\in\R$.\par
            We first describe $\mathbf{f}(U)$. Since $x=t$ for all $(x,y)\in U$, $\e[x]=\e[t]$. Since $y$ ranges over $\R$ as $(x,y)$ ranges over $U$, the image of $U$ under $\cos y$ is $[-1,1]$. Similarly, the image of $U$ under $\sin y$ is $[-1,1]$. Thus, $\mathbf{f}(U)=(f_1(U),f_2(U))$ where the two components are given by
            \begin{align*}
                f_1(U) &= [-\e[t],\e[t]]&
                f_2(U) &= [-\e[t],\e[t]]
            \end{align*}
            We now describe $\mathbf{f}(V)$. Since $x$ ranges over $\R$ as $(x,y)$ ranges over $V$, the image of $V$ under $\e[x]$ is $(0,\infty)$. Since $y=t$ for all $(x,y)\in U$, $\cos y=0$ if $t=\frac{\pi}{2}+n\pi$, $\cos y>0$ if $t\in(-\frac{\pi}{2}+2\pi n,\frac{\pi}{2}+2\pi n)$, and $\cos y<0$ if $t\in(\frac{\pi}{2}+2\pi n,\frac{3\pi}{2}+2\pi n)$ ($n\in\Z$). Similarly, $\sin y=0$ if $t=n\pi$, $\sin y>0$ if $t\in(2n\pi,(2n+1)\pi)$, and $\sin y<0$ if $t\in((2n-1)\pi,2n\pi)$ ($n\in\Z$). Thus, $\mathbf{f}(V)=(f_1(V),f_2(V))$ where the two components are given by
            \begin{align*}
                f_1(V) &=
                \begin{cases}
                    \{0\} & t=\frac{\pi}{2}+n\pi\\
                    (0,\infty) & -\frac{\pi}{2}+2\pi n<t<\frac{\pi}{2}+2\pi n\\
                    (0,-\infty) & \frac{\pi}{2}+2\pi n<t<\frac{3\pi}{2}+2\pi n
                \end{cases}&
                f_2(V) &=
                \begin{cases}
                    \{0\} & t=n\pi\\
                    (0,\infty) & 2n\pi<t<(2n+1)\pi\\
                    (0,-\infty) & (2n-1)\pi<t<2n\pi
                \end{cases}
            \end{align*}
            where $n\in\Z$ in every occurrence.
        \end{proof}
    \end{enumerate}
\end{enumerate}




\end{document}